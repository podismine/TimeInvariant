network:
  feature_dim: 1024
  depth: 12
  heads: 8
  dim_feedforward: 2048
  # Specify a folder containing a pre-trained model to fine-tune. If training from scratch, pass None.
  projection_head:
    mlp_hidden_size: 512
    projection_size: 128

saving:
  log_dir: output_log_alldata_d12_h8_1024_t15
  checkpoint_dir: output_checkpoint_alldata_d12_h8_1024_t15
  n_epochs: 10

trainer:
  batch_size: 96
  m: 0.996 # momentum update
  checkpoint_interval: 5000
  max_epochs: 10000
  num_workers: 8
  acc_lambda: 0.1
  mse_lambda: 1.0
  warmup_epochs: 1

optimizer:
  lr: 0.0003
  weight_decay: 0.0001

data:
  time_len: 15
