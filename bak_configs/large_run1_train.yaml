network:
  feature_dim: 1024
  depth: 12
  heads: 10
  dim_feedforward: 2048
  # Specify a folder containing a pre-trained model to fine-tune. If training from scratch, pass None.
  projection_head:
    mlp_hidden_size: 512
    projection_size: 128
  drop: 0.2
saving:
  log_dir: output_log_abide1_d24_h16_1024_t15_train
  checkpoint_dir: output_checkpoint_abide1_d24_h16_1024_t15_train
  n_epochs: 10

trainer:
  batch_size: 64
  m: 0.996 # momentum update
  checkpoint_interval: 5000
  max_epochs: 5000
  num_workers: 8
  acc_lambda: 0.1
  mse_lambda: 10
  warmup_epochs: 1

optimizer:
  lr: 0.0003
  weight_decay: 0.00005

data:
  path: /data5/yang/large/run1_abide1_train/
  time_len: 15
